# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from pathlib import Path
from typing import List, Dict
import concurrent.futures
from google.cloud import storage
from google.cloud.exceptions import NotFound
from google.oauth2 import service_account
from ailib.env_config import GCS_PROJECT_ID, SA_KEY_FILE_PATH, GCS_AUTH_TYPE


def _get_gcs_credentials():
  """
  æ ¹æ® GCS_AUTH_TYPE è·å– GCS å‡­æ®
  """
  credentials = None
  if GCS_AUTH_TYPE == "SA":
    if SA_KEY_FILE_PATH and os.path.exists(SA_KEY_FILE_PATH):
      credentials = service_account.Credentials.from_service_account_file(
          SA_KEY_FILE_PATH,
          scopes=['https://www.googleapis.com/auth/cloud-platform']
      )
    else:
      print(f"è­¦å‘Š: GCS SA è®¤è¯æ¨¡å¼ä½†æœåŠ¡è´¦æˆ·æ–‡ä»¶ä¸å­˜åœ¨: {SA_KEY_FILE_PATH}")
      print("å›é€€åˆ°é»˜è®¤è®¤è¯æ¨¡å¼ (ADC)")
  else:
    print(f"GCS ä½¿ç”¨é»˜è®¤è®¤è¯æ¨¡å¼ (ADC)")

  return credentials


def init_client():
  """
  åˆå§‹åŒ– GCS å®¢æˆ·ç«¯
  æ ¹æ® GCS_PROJECT_ID å’Œ GCS_AUTH_TYPE é…ç½®
  """
  project_id = GCS_PROJECT_ID
  credentials = _get_gcs_credentials()

  storage_client = storage.Client(
      project=project_id,
      credentials=credentials
  )
  return storage_client


def download_gcs_object(gcs_uri: str,
    local_destination_path: str = None) -> bool:
  """
  ä» Google Cloud Storage ä¸‹è½½ä¸€ä¸ªå¯¹è±¡åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€‚

  å‚æ•°:
      gcs_uri (str): GCS å¯¹è±¡çš„å®Œæ•´ URIï¼Œæ ¼å¼ä¸º "gs://bucket-name/path/to/object".
      local_destination_path (str, optional): æœ¬åœ°ä¿å­˜è·¯å¾„ï¼ŒåŒ…æ‹¬æ–‡ä»¶åã€‚
                                              å¦‚æœä¸º Noneï¼Œå°†ä½¿ç”¨ GCS å¯¹è±¡çš„åç§°ï¼Œå¹¶ä¿å­˜åœ¨å½“å‰å·¥ä½œç›®å½•ã€‚

  è¿”å›:
      bool: å¦‚æœä¸‹è½½æˆåŠŸåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
  """
  if not gcs_uri.startswith("gs://"):
    print(f"é”™è¯¯ï¼šæ— æ•ˆçš„ GCS URI '{gcs_uri}'ã€‚å®ƒå¿…é¡»ä»¥ 'gs://' å¼€å¤´ã€‚")
    return False

  try:
    # è§£æ GCS URI
    path_parts = gcs_uri[5:].split("/", 1)
    if len(path_parts) < 2 or not path_parts[0] or not path_parts[1]:
      print(
        f"é”™è¯¯ï¼šæ— æ•ˆçš„ GCS URI æ ¼å¼ '{gcs_uri}'ã€‚æœŸæœ›æ ¼å¼: gs://bucket-name/object-path")
      return False

    bucket_name = path_parts[0]
    blob_name = path_parts[1]

    # å¦‚æœ blob_name ä»¥ '/' ç»“å°¾ï¼Œè¯´æ˜å®ƒå¯èƒ½æ˜¯ä¸€ä¸ªâ€œæ–‡ä»¶å¤¹â€å ä½ç¬¦ï¼Œæˆ–è€…ç”¨æˆ·é”™è¯¯åœ°æŒ‡å®šäº†
    # download_to_filename ä¸èƒ½ä¸‹è½½è¿™æ ·çš„å¯¹è±¡ï¼Œå®ƒéœ€è¦ä¸€ä¸ªå…·ä½“çš„æ–‡ä»¶åã€‚
    if blob_name.endswith('/'):
      print(
        f"é”™è¯¯ï¼šå¯¹è±¡è·¯å¾„ '{blob_name}' çœ‹èµ·æ¥åƒä¸€ä¸ªç›®å½•ã€‚è¯·æŒ‡å®šä¸€ä¸ªå…·ä½“çš„æ–‡ä»¶å¯¹è±¡ã€‚")
      return False

    # åˆå§‹åŒ– GCS å®¢æˆ·ç«¯
    storage_client = init_client()

    # è·å– bucket å’Œ blob
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)

    # ç¡®å®šæœ¬åœ°ç›®æ ‡è·¯å¾„
    if local_destination_path is None:
      local_destination_path = os.path.basename(blob_name)
      if not local_destination_path:  # é˜²æ­¢ blob_name æ˜¯ç±»ä¼¼ "some/path/" ååˆè¢«é”™è¯¯å¤„ç†åˆ°è¿™é‡Œ
        print(f"é”™è¯¯ï¼šæ— æ³•ä» GCS è·¯å¾„ '{blob_name}' æ¨æ–­å‡ºæœ¬åœ°æ–‡ä»¶åã€‚")
        return False
    else:
      # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
      local_dir = os.path.dirname(local_destination_path)
      if local_dir and not os.path.exists(local_dir):
        os.makedirs(local_dir)
        print(f"åˆ›å»ºç›®å½•: {local_dir}")

    print(
      f"æ­£åœ¨ä¸‹è½½ gs://{bucket_name}/{blob_name} åˆ° {local_destination_path}...")

    blob.download_to_filename(local_destination_path)

    print(f"æ–‡ä»¶æˆåŠŸä¸‹è½½åˆ°: {local_destination_path}")
    return True

  except NotFound:
    print(
      f"é”™è¯¯ï¼šåœ¨ GCS ä¸Šæœªæ‰¾åˆ°å¯¹è±¡ gs://{bucket_name}/{blob_name} æˆ–å­˜å‚¨æ¡¶ä¸å­˜åœ¨ã€‚")
    return False
  except Exception as e:
    print(f"ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    return False


def upload_gcs_object(bucket_name, destination_blob_name,
    source_file_name) -> bool:
  try:
    # åˆå§‹åŒ– GCS å®¢æˆ·ç«¯
    storage_client = init_client()

    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    # Optional: set a generation-match precondition to avoid potential race conditions
    # and data corruptions. The request to upload is aborted if the object's
    # generation number does not match your precondition. For a destination
    # object that does not yet exist, set the if_generation_match precondition to 0.
    # If the destination object already exists in your bucket, set instead a
    # generation-match precondition using its generation number.
    generation_match_precondition = 0

    blob.upload_from_filename(source_file_name,
                              if_generation_match=generation_match_precondition)

    print(
        f"File {source_file_name} uploaded to {destination_blob_name}."
    )
    return True
  except Exception as e:
    print(f"ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    return False


def blob_metadata(bucket_name, blob_name):
  # åˆå§‹åŒ– GCS å®¢æˆ·ç«¯
  storage_client = init_client()
  bucket = storage_client.bucket(bucket_name)

  # Retrieve a blob, and its metadata, from Google Cloud Storage.
  # Note that `get_blob` differs from `Bucket.blob`, which does not
  # make an HTTP request.
  blob = bucket.get_blob(blob_name)
  return blob


def _upload_task(local_file: str, bucket: storage.Bucket, gcs_path: str):
  """
  å•ä¸ªæ–‡ä»¶çš„ä¸Šä¼ ä»»åŠ¡ï¼Œç”¨äºåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œã€‚

  :return: ä¸€ä¸ªå…ƒç»„ (local_file, status_message)
  """
  try:
    file_name = Path(local_file).name
    # æ‹¼æ¥åœ¨ GCS ä¸­çš„å®Œæ•´å¯¹è±¡è·¯å¾„
    # ä¾‹å¦‚: 'videos/my_project/filename_part_1.mp4'
    object_name = f"{gcs_path}/{file_name}" if gcs_path else file_name
    object_gcs_name = f"gs://{bucket.name}/{object_name}"
    blob = bucket.blob(object_name)
    # 1. åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if blob.exists():
      status = "Skipped: Already exists"
      print(f"â­ï¸ {file_name}: {status}")
      return local_file, object_gcs_name, status

    # 2. å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä¸Šä¼ 
    print(f"ğŸ”¼ {file_name}: Uploading...")
    blob.upload_from_filename(local_file, timeout=6000)
    status = "Uploaded"
    print(f"âœ… {file_name}: {status}")
    return local_file, object_gcs_name, status

  except Exception as e:
    status = f"Failed: {e}"
    print(f"âŒ {file_name}: {status}")
    return local_file, "", status


def upload_parts_to_gcs_parallel(
    local_files: List[str],
    bucket_name: str,
    object_path: str,
    max_workers: int = 8
) -> Dict[str, List[str]]:
  """
  å¹¶è¡Œåœ°å°†ä¸€ç³»åˆ—æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ° Google Cloud Storageã€‚

  :param local_files: è¦ä¸Šä¼ çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
  :param bucket_name: GCS å­˜å‚¨æ¡¶çš„åç§°ã€‚
  :param object_path: æ–‡ä»¶åœ¨å­˜å‚¨æ¡¶ä¸­å­˜æ”¾çš„è·¯å¾„ï¼ˆ"æ–‡ä»¶å¤¹"ï¼‰ã€‚
  :param max_workers: å¹¶è¡Œä¸Šä¼ çš„æœ€å¤§çº¿ç¨‹æ•°ã€‚
  :return: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå€¼æ˜¯ä¸Šä¼ çŠ¶æ€ã€‚
  """

  storage_client = init_client()
  bucket = storage_client.bucket(bucket_name)
  upload_results = {}

  # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œä¸Šä¼ 
  with concurrent.futures.ThreadPoolExecutor(
      max_workers=max_workers) as executor:
    # æäº¤æ‰€æœ‰ä¸Šä¼ ä»»åŠ¡
    future_to_file = {
        executor.submit(_upload_task, local_file, bucket,
                        object_path): local_file
        for local_file in local_files
    }

    print(
      f"\n--- å¼€å§‹å¹¶è¡Œä¸Šä¼ åˆ° GCS Bucket: gs://{bucket_name}/{object_path} ---")

    # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
    for future in concurrent.futures.as_completed(future_to_file):
      try:
        file_path, object_name, status = future.result()
        upload_results[file_path] = [file_path, object_name, status]
      except Exception as e:
        # ä¸€èˆ¬ _upload_task å†…éƒ¨ä¼šæ•è·å¼‚å¸¸ï¼Œè¿™é‡Œä½œä¸ºæœ€åçš„ä¿éšœ
        file_path = future_to_file[future]
        upload_results[file_path] = [file_path, object_name,
                                     f"Failed in executor: {e}"]

  return upload_results

# bucket_name = "gemini-oolongz"
# blob_name = "movie_metadata.txt"
# blob = blob_metadata(bucket_name, blob_name)
# print(blob)
